# üìÑ Document Intelligence Assistant

A smart web-based assistant that allows users to upload documents (`PDF`, `DOCX`, `TXT`), ask questions, and get meaningful answers or summaries using modern AI models and semantic search.

---

## üìÅ Folder Structure

### üß† Backend Folder Structure (/Backend) 
``` 
Backend/
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îî‚îÄ‚îÄ routes.py                    # FastAPI route definitions (upload, ask, summarize, reset)
‚îÇ
‚îú‚îÄ‚îÄ db/ 
‚îÇ   ‚îî‚îÄ‚îÄ chroma_store.py              # ChromaDB vector store setup and operations
‚îÇ
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ document_handler.py         # File parsing (PDF, DOCX, TXT)
‚îÇ   ‚îú‚îÄ‚îÄ embedder.py                 # Embedding generation using sentence-transformers
‚îÇ   ‚îú‚îÄ‚îÄ llm_handler.py              # LLM calling logic (Groq/OpenRouter fallback)
‚îÇ   ‚îú‚îÄ‚îÄ pipeline.py                 # Core ingestion & chunking pipeline
‚îÇ   ‚îî‚îÄ‚îÄ retriever.py                # Semantic search (retrieves relevant document chunks)
‚îÇ
‚îú‚îÄ‚îÄ temp_uploads/                   # Temporary file storage during upload
‚îÇ   ‚îú‚îÄ‚îÄ Edusphere paper.pdf
‚îÇ   ‚îú‚îÄ‚îÄ Honors Final Assessment.docx
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ answer_modifier.py         # Format and clean LLM responses
‚îÇ   ‚îú‚îÄ‚îÄ logger.py                  # Structured logging configuration
‚îÇ   ‚îú‚îÄ‚îÄ memory_json.py            # Store/retrieve Q&A memory per session in JSON
‚îÇ   ‚îî‚îÄ‚îÄ splitter.py               # Chunking logic for large text
‚îÇ
‚îú‚îÄ‚îÄ conversation_memory.json       # Stores conversation history for sessions
‚îú‚îÄ‚îÄ main.py                        # FastAPI entrypoint and route registration
‚îú‚îÄ‚îÄ app.log                        # Log file for backend operations
‚îú‚îÄ‚îÄ .env                           # Backend environment variables
‚îú‚îÄ‚îÄ .gitignore                     # Ignore unnecessary files in Git
‚îî‚îÄ‚îÄ requirements.txt               # Python dependencies

```

### üíª Frontend Folder Structure (/Frontend)

```
Frontend/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ layout.tsx                     # Shared layout for all pages
‚îÇ   ‚îî‚îÄ‚îÄ page.tsx                       # Main document assistant interface
‚îÇ
‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îú‚îÄ‚îÄ ui/                            # Reusable UI primitives
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ alert.tsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ badge.tsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ button.tsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ card.tsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ input.tsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ separator.tsx
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ document-upload.tsx           # File upload component
‚îÇ   ‚îú‚îÄ‚îÄ question-form.tsx             # Input box for asking questions
‚îÇ   ‚îú‚îÄ‚îÄ reset-memory-button.tsx       # Button to clear session memory
‚îÇ   ‚îú‚îÄ‚îÄ response-display.tsx          # Shows assistant's response
‚îÇ   ‚îî‚îÄ‚îÄ summarize-button.tsx          # Button to trigger document summarization
‚îÇ
‚îú‚îÄ‚îÄ hooks/
‚îÇ   ‚îî‚îÄ‚îÄ use-document-assistant.ts     # Custom hook managing document assistant logic
‚îÇ
‚îú‚îÄ‚îÄ lib/
‚îÇ   ‚îú‚îÄ‚îÄ api.ts                         # API interaction functions (ask, upload, summarize)
‚îÇ   ‚îú‚îÄ‚îÄ types.ts                       # TypeScript types and interfaces
‚îÇ   ‚îî‚îÄ‚îÄ utils.ts                       # Utility helpers
‚îÇ
‚îú‚îÄ‚îÄ src/styles/
‚îÇ   ‚îî‚îÄ‚îÄ globals.css                    # Tailwind base styles and global CSS
‚îÇ
‚îú‚îÄ‚îÄ .env.example                       # Example environment config
‚îú‚îÄ‚îÄ .env.local                         # Local environment config
‚îú‚îÄ‚îÄ .gitignore                         # Git ignored files
‚îú‚îÄ‚îÄ components.json                    # Shadcn UI config (if used)
‚îú‚îÄ‚îÄ next-env.d.ts                      # TypeScript declaration for Next.js
‚îú‚îÄ‚îÄ package.json                       # Project config and dependencies
‚îú‚îÄ‚îÄ package-lock.json                  # Lockfile for reproducible installs
‚îú‚îÄ‚îÄ postcss.config.js                  # PostCSS config
‚îú‚îÄ‚îÄ tailwind.config.js                 # Tailwind CSS config
‚îî‚îÄ‚îÄ tsconfig.json                      # TypeScript compiler options

```
---

## üöÄ Features

- üì§ Upload support for PDF, DOCX, and TXT
- üîç Semantic search over document content using embeddings
- üí¨ Ask questions and receive contextual, accurate answers
- üß† Maintains conversation memory per document session
- üßæ Summarization in clean markdown bullet-point format
- üîÑ API fallback support (Groq ‚Üí OpenRouter)

---

## üõ†Ô∏è Tech Stack

| Layer         | Tech                                                                 |
|---------------|----------------------------------------------------------------------|
| Frontend      | React + Tailwind (Next.js)                                    |
| Backend       | Python + FastAPI                                                     |
| Embeddings    | `sentence-transformers/all-MiniLM-L6-v2` or `hkunlp/instructor-xl`   |
| Vector Store  | Chroma DB                                                            |
| LLMs          | Groq (`llama3-70b-8192`), OpenRouter (`gpt-4o-mini`)                 |
| File Parsing  | `pdfplumber`, `python-docx`                                          |

---

## üì¶ Setup Instructions

### Backend Setup (FastAPI + ChromaDB + LLM APIs)

```bash
# Step 0: Cloning instructions
git clone https://github.com/Jatin-35/Document-Intelligence-Assistant.git
cd Document-Intelligence-Assistant

# Step 1: Create a virtual environment using UV (faster than pip)
uv venv .venv

# Step 2: Activate the virtual environment
# On Unix or MacOS:
source .venv/bin/activate

# On Windows (PowerShell):
.venv\Scripts\Activate.ps1

# Step 3: Install Python dependencies listed in requirements.txt
uv pip install -r requirements.txt

# Step 4: Set up environment variables (copy the example and add your keys)
 .env

# Open the .env file and add:
# GROQ_API_KEY=your_groq_key
# OPENROUTER_API_KEY=your_openrouter_key

# Step 5: Run the FastAPI server in development mode
uvicorn main:app --reload
```
- Dependencies: fastapi, uvicorn, sentence-transformers, chromadb, pdfplumber, python-docx, and others.

### üíª Frontend

```bash
# Step 1: Move into the frontend directory
cd frontend

# Step 2: Install all frontend dependencies
npm install

# Step 3: Run the frontend development server
npm run dev

# Step 4 (only if setting up fresh from scratch):

# 4.1 - Install Tailwind CSS & its dependencies
npx tailwindcss init -p

# 4.2 - Add global styles and content paths
# In tailwind.config.js:
# content: ["./app/**/*.{js,ts,jsx,tsx}", "./components/**/*.{js,ts,jsx,tsx}"]

# 4.3 - Initialize shadcn/ui
npx shadcn-ui@latest init

# Follow prompts and select:
# > App directory: yes
# > Typescript: yes
# > Tailwind CSS: yes
# > Component path: ./components/ui

# 4.4 - Install any specific component (e.g. Button)
npx shadcn-ui@latest add button
```
 - Libraries used: next, tailwindcss, shadcn/ui, lucide-react, clsx, and other TypeScript utilities.

---
## ‚úÖ Dataset Information

This project does not rely on any external or pre-built datasets. All data is provided dynamically by users at runtime when they upload their documents. The system processes documents in **PDF, DOCX, and TXT** formats.

### **Source of Data**
- Documents are uploaded by users through the web interface.
- Files are stored temporarily in the directory:  
  **`/Backend/temp_uploads/`**
- No document is permanently stored unless explicitly chosen by the user.


### **Data Size**
- File size varies depending on user uploads.
- The system is optimized to handle both small and large documents using chunking and semantic indexing.

---

## üßæ **Preprocessing Steps Applied**
Each uploaded document follows a structured preprocessing pipeline:

#### **1. Text Extraction**
- `pdfplumber` for PDF parsing  
- `python-docx` for DOCX parsing  
- Standard text loaders for TXT files  

#### **2. Cleaning & Normalization**
- Removal of special characters  
- Standardization of whitespace  
- Basic text normalization to improve chunk quality  

#### **3. Chunking into Semantic Units**
- Document split into meaningful segments  
- Overlapping window strategy to maintain context  
- Chunk sizes tuned for efficient retrieval and LLM constraints  

#### **4. Embedding Generation**
Embeddings generated via one of the following:
- `sentence-transformers/all-MiniLM-L6-v2`
- `hkunlp/instructor-xl` (optional for higher accuracy)

#### **5. Vector Indexing**
- Chunks are stored in a **ChromaDB** collection  
- Metadata stored with each entry:
  - `doc_id`
  - `session_id`

This preprocessing pipeline ensures high-quality semantic search, accurate retrieval, and grounded responses formatted by the LLM.

<img width="546" height="742" alt="image" src="https://github.com/user-attachments/assets/87350c20-4288-44e5-a531-eb43cde42791" />

---

## Architecture Flow Diagram

<img width="600" height="600" alt="üìÑ Document Intelligence Assistant - visual selection (1)" src="https://github.com/user-attachments/assets/eb5b1c16-18b3-4b7e-9c79-d83d88380b80" />

> ‚ÄúThis flow diagram shows how the Document Intelligence Assistant processes uploaded documents through splitting, embedding, retrieval, and LLM reasoning to generate accurate, context-grounded answers.‚Äù
---
## üß™ Usage

1. Open the web interface.
2. Upload a supported document (`PDF`, `DOCX`, or `TXT`).
3. Ask natural language questions like:
   - ‚ÄúWhat is this document about?‚Äù
   - ‚ÄúSummarize the key points.‚Äù
4. View highlighted document chunks with confident LLM-generated answers.
5. Click **üìÑ Summarize Document** to generate a clean bullet-point summary.


Below is an example illustrating how the Document Intelligence Assistant processes a document, understands a user query, and generates grounded, context-aware responses.

---

### üìò Sample Input Document

**File:** *‚ÄúAI in Healthcare.pdf‚Äù*

**Content (excerpt):**
> ‚ÄúAlthough AI is transforming medical diagnostics, several challenges remain, including regulatory compliance, patient data privacy, limited availability of annotated medical datasets, and the high computational resources required for training large models.‚Äù

### ‚ùì Sample User Query

What are the main challenges mentioned?

### ‚úÖ Expected Output

Based on semantic retrieval of the most relevant chunks and LLM-based reasoning, the system would generate a structured answer similar to the following:

#### **Answer**
The document highlights several key challenges associated with the use of AI in healthcare:

- **Regulatory compliance issues**  
- **Data privacy and protection concerns**  
- **Limited availability of high-quality, annotated medical datasets**  
- **High computational costs** for training and deploying advanced AI models  

### üß© Supporting Evidence (Retrieved Chunks)

The assistant also returns the document segments used to answer the query, along with the confidence score:


---

## üåü Advanced Features

- **API Integration (Fallback)** ‚Äì Automatically switches from Groq to OpenRouter when rate-limited (HTTP 429).
- **Conversation Memory** ‚Äì Maintains Q&A history per document session to preserve context and improve relevance.
- **Summarization** ‚Äì Converts raw document content into clean, markdown-formatted bullet points using LLMs.

---

üë®‚Äçüíª Author
Developed  by [Jatin Jasrotia](https://github.com/Jatin-35), Ankitkumar Patel, Madhav Gupta



